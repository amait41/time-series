---
title: "Série Chronologique - Projet"
output:
  pdf_document: 
    toc: yes
    fig_width: 7
    fig_height: 4
    fig_caption: yes
    number_sections: yes
    keep_tex: yes
---

```{r warning=FALSE, echo=FALSE, results='hide'}
# library(TSA)
library(tseries)
library(forecast)
source(file = "functions.R")
```

# Introduction

L'objet d'étude de ce projet est une série temporelle décrivant le nombre de commandes par jour d'une entreprise angevine de livraison de repas à domicile. Cette série est découpée en un jeu d'entrainement (train set) de 280 observations et un jeu de test (test set) de 21 observations. Notre objectif sera d'étudier cette série et de construire un modèle de la famille des modèles $ARMA$ afin de prédire les valeurs du jeu de test soit 21 jours.

```{r}
data = read.csv('orders.csv')
trainset = ts(data$n_orders, start=1, end=40, frequency=7)
testset = ts(data$n_orders, start=41, end=44, frequency=7)
plot(trainset, type='l', xlab="Période", ylab="# commades", main="Nombre de commandes par jour (trainset)")
```

# Analyse visuelle

Cette série présente plusieurs caractéristiques remarquables.

Il y a une forte saisonalité sur les jours de la semaine. En effet, le nombre de commandes croit du dimanche au vendredi puis décroît le samedi en moyenne.

La série semble composée de deux voir trois régime. Un premier lors des 18 premières semaines avant une tendance haussière, une saisonnalité très marquée et une variance relativement grande. Puis, vient 16 semaines de tendance baissière, avec une saisonnalité moins marquée et une plus faible variance. Enfin, les 5 dernières semaines semble reprendre quelque peu le schéma de premières semaine avec un tendance à la hausse, une saisonnalité marquée et une variance qui augmente.

```{r,  echo=FALSE, fig.width=15, fig.height=4}
layout(matrix(c(1, 2), nrow=1, ncol=2, byrow=TRUE))
# Segmentation
t1 = 1:126; X1 = trainset[t1]
t2 = 127:246; X2 = trainset[t2]
t3 = 247:280; X3 = trainset[t3]
lm1 = lm(X1~t1)
lm2 = lm(X2~t2)
lm3 = lm(X3~t3)
# print(summary(lm1))
# print(summary(lm2))
# print(summary(lm3))
plot(t1, X1, type='l', xlab="Index", ylab="# commandes", main="Segmentation de la série temporelle", xlim=c(0,length(trainset)))
lines(t2, X2, type="l", col="blue")
lines(t3, X3, type="l", col="red")
lines(t1, lm1$coefficients[1]+ t1 * lm1$coefficients[2])
lines(t2, lm2$coefficients[1]+ t2 * lm2$coefficients[2], col="blue")
lines(t3, lm3$coefficients[1]+ t3 * lm3$coefficients[2], col="red")

# Saisonalité
help(seasonplot)
seasonplot(trainset, type="l", main="Graphique saisonnier", xlab="Jour", ylab="# commandes", season.labels=c("Dimanche", "Lundi", "Mardi", "Mercredi", "Jeudi", "Vendredi", "Samedi"))
```

Les modèles $ARMA$ font l'hypothèse que la série temporelle est stationnaire (du second ordre), c'est à dire que sa moyenne et sa covariance sont invariantes par translation dans le temps. Pour rendre cette série stationnaire, nous allons utiliser plusieurs approches telles que la décomposition (partie 2) et la différentiation (partie 3). Puis nous établirons des modèles de la famille $ARIMA$ à l'aide de la fonction `auto.arima` et `checkup_res` (partie 3) avant d'évaluer nos modèles sur le jeu de donnée test avec des critères prédictifs (partie 4).

# Décomposition

Afin de valider cette hypothèse, il est possible d'appliquer la procédure suivante. Dans un premier temps, on applique sur la série un opérateur moyenne mobile $M_m(B) = \frac{1}{2m+1} \sum\limits_{n=1}^{m} B^{-k}$  qui élimine  les tendances $\tau$-périodiques avec $B$ l'opérateur de retard tel que $B^kX_t=X_{t-k}$. Puis, on estime par régression la tendance sur la série filtrée. Enfin, on estime la composante saisonnière en effectuant la moyenne des périodes et en dupliquant ce résultat autant de fois que nécéssaire. On obtient la décomposition ci-dessous.

```{r, fig.width=7, fig.height=5, echo=FALSE}
decomposition = decompose(trainset)
# plot(decomposition)

decomp.plot2 <- function(x, main)
{
    if(missing(main))
	main <- paste("Decomposition of", x$type, "time series")
    plot(cbind(observed = x$random + if (x$type == "additive")
        x$trend + x$seasonal
    else x$trend * x$seasonal, trend = x$trend, seasonal = x$seasonal,
        random = x$random), main = main)
}
decomp.plot2(decomposition, main="Décomposition additive de la série")
# decomposition_stl = stl(trainset, s.window=7)
# plot(decomposition_stl)
```
Pour pouvoir construire un modèle de prédiction sur la base de ces résultats, nous allons appliquer une regression linéaire sur la tendance puis y ajouter la composante saisonnière.

```{r, fig.width = 12, fig.height=4, echo=FALSE, warning=FALSE}
layout(matrix(c(1, 2), nrow=1, ncol=2, byrow=TRUE))
t = 1:length(decomposition$trend)
t2 = t^2
t3 = t^3
t4 = t^4
reg_lin1 = lm(decomposition$trend ~ t)
reg_lin2 = lm(decomposition$trend ~ t + t2)
reg_lin3 = lm(decomposition$trend ~ t + t2 + t3)
reg_lin4 = lm(decomposition$trend ~ t + t2 + t3 + t4)

plot(t, as.numeric(decomposition$trend), type="l", lwd=1.5, main="Regression sur la tendance", ylab="decomp$trend", xlab="Temps")
lines(reg_lin1$fitted.values, type="l")
lines(reg_lin2$fitted.values, type="l", col="blue")
lines(reg_lin3$fitted.values, type="l", col="red")
lines(reg_lin4$fitted.values, type="l", col="green")
legend("topright", legend=c("ordre 1", "ordre 2", "ordre 3", "ordre 4"), col=c("black", "blue", "red", "green"), lw=1)

decomp_ = decomp(trainset, testset)
plot(decomp_$train, main="Régression + composante saisonière", xlab="Périodes", ylab="decomp$trend + decomp$seasonal")
```
Visuellement, les regression d'ordre 3 et 4 donnent de très bon résultat avec un R² ajusté de 0.88. Le principe de parcimonie nous pousse à garder la régression linéaire d'ordre 3. Analysons à présent les résidus de la décomposition.

```{r, fig.width=8, fig.height=4, echo=FALSE, warning=FALSE}
plot(decomposition$random, ylab="", xlab="Période", main="Résidus de la décomposition")
```
Il semble y avoir une légère sésonalité dans les résidus.

En plus de l'aspect visuel, les tests suivants permettent d'évaluer l'hypothèse de stationarité :<br>
- ADF avec $\mathcal{H}_0$ : "la trajectoire est issue d'un processus non stationnaire" contre $\mathcal{H}_1=\bar{\mathcal{H}_0}$.<br>
- KPSS avec $\mathcal{H}_0$ : "la trajectoire est issue d'un processus stationnaire" contre $\mathcal{H}_1=\bar{\mathcal{H}_0}$.<br>

```{r, results="hide", echo=FALSE}
res_decomp = decomposition$random[which(!is.na(decomposition$random))]
adf.test(res_decomp)
kpss.test(res_decomp)
```
| Test | Statistique | p-value |
|------|-------------|---------|
| ADF  | -12.135     | <0.01   |
| KPSS | 0.020884    | >0.1    |
<!-- |------------------------------| -->
<!-- |<center>>Tests de stationarités</center> -->

Les tests s'accordent. Le test ADF rejette l'hypothèse de non stationarité (p-value<<0.05) alors que le test KPSS ne rejette pas l'hypothèse de stationarité (p-value>>0.05). Nous avons donc obtenu des résidus stionnaires d'après ces tests.

Présentons maintenant deux outils indispensable à l'étude des processus ARMA: l'ACF et la PAFC:<br>
- L'AFC représente les autocorrélation entre deux valeurs distantes de $h$ dans le temps. Une autocorrélation nulle à partir d'un rang $q+1$ est caractéristique d'un processus $MA(q)$.<br>
- L'autocorrélogramme PAFC représente quand à lui les corrélation "pur" entre deux valeurs distantes de $h$ dans le temps, c'est à dire entre lesquelles on a supprimé l'influence linéaire des valeurs intermédiaires. Une PACF avec des pics dans le couloir de non-significativité à partir du rang $p+1$ est caractéristique d'un processus $AR(p)$.

De plus, on justifie une tentative de modélisation par un processus $ARMA$ lorsque la série est considérée comme stationnaire et que ses ACF et PACF empiriques montrent une décroissance rapide.

```{r fig.width=12, fig.height=4, echo=FALSE}
layout(matrix(c(1, 2), nrow=1, ncol=2, byrow=TRUE))
acf(res_decomp, main="")
pacf(res_decomp, main="")
```
L'ACF présente des pics périodiques majoritairement significatifs avec une faible décroissance. On observe une corrélation élevée pour les lag multiple de 7. La PACF possède des corrélations partielles significatives aux lags 1 à 6. Ainsi, il est clair que nous encore avons une saisonalité d'ordre 7.

En conclusion, les résidus du modèle de decomposition ne sont pas stationnaires et présentent une saisonalité d'ordre 7.

# Différenciation

## Différentiation simple

Afin de rendre une série stationnaire, il est possible d'intégrer cette dernière en lui appliquant un filtre de la forme $(I-B)$.  On regarde alors si $\Delta X_t = (I-B)X_t$ est stationnaire.

Cette méthodes ne donne pas de résultats satisfaisant sur les résidus de la décomposition (cf. Annexe 1). Nous allons alors appliquer cette méthode sur la série originale.

Les tests de stationarité nous incitent à penser que la série différanciée $(\Delta X_t)$ est pas stationnaire.

```{r, echo=FALSE, results='hide', warning=FALSE}
diff1 = diff(trainset, lag=1)
adf.test(diff1)
kpss.test(diff1)
```
| Test      | Statistique | p-value   |
|-----------|-------------|-----------|
| ADF       | -11.875     | <0.01     |
| KPSS      | 0.06673     | >0.1      |
<!-- | Ljung-Box | 241.24      | 2.2e-16   | -->
<!-- | Shapiro   | 0.97117     | 2.598e-05 | -->

Néanmoins, l'ACF et la PACF présentent repectivement des autocorrélationS très significatives sur les lags 7, 14, 21, 28 et 1 à 7.

```{r, fig.width=12, fig.height=4, echo=FALSE}
layout(matrix(c(1, 2), nrow=1, ncol=2, byrow=TRUE))
acf(diff1, main="")
pacf(diff1, main="")
```
Nous devons conclure que les incréments ne sont pas stationnaires car ils contients encore une composante saisonnière.

## Différentiation saisonnière

.La différentiation saisonnière consiste à appliquer un filtre de la forme $(I-B^s)$ à notre série. On obtient alors les incréments saisonniers $(\Delta_s X_t = (I-B^s)X_t)$. L'application de ce filtre sur la série semble tout indiqué car il élimine les tendances s-périodique. Les tests ADF et KPPSS s'accordent sur l'hypothèse de stationarité. 

| Test      | Statistique | p-value   |
|-----------|-------------|-----------|
| ADF       | -11.875     | <0.01     |
| KPSS      | 0.06673     | >0.1      |

```{r,  echo=FALSE, warning=FALSE}
diff7 = diff(trainset, lag=7)
adf.test(diff7)
kpss.test(diff7)
```
L'ACF montre une décroissance rapide avec des pics significatifs en 1, 4 et 7. La PACF possède aussi une décroissance rapide et des pics significatifs en 1 et 7. On pourra alors tenter une modélisation $ARMA$ sur la série $Y_t = \Delta_7 X_t = (I-B^7) X_t$.

```{r, fig.width=12, fig.height=4, echo=FALSE}
layout(matrix(c(1, 2), nrow=1, ncol=2, byrow=TRUE))
acf(diff7, lag=40)
pacf(diff7, lag=40)
```
Essayons de différencier à nouveau la série, c'est à dire d'appliquer le filtre $(I-B)(I-B^7)$ sur la série.

Les tests vont encore dans le sens de la stationnarité.

| Test      | Statistique | p-value   |
|-----------|-------------|-----------|
| ADF       | -11.875     | <0.01     |
| KPSS      | 0.06673     | >0.1      |

```{r,  echo=FALSE, warning=FALSE}
ddiff7 = diff(diff(trainset, lag=7))
adf.test(ddiff7)
kpss.test(ddiff7)
```
L'ACF et la PACF donnent de moins bon résultat dans le sens où l'ACF donne des résultats similaires mais la PACF décroit beaucoup moins vite et possède d'avantage de pics significatifs.

```{r, fig.width=12, fig.height=4, echo=FALSE}
layout(matrix(c(1, 2), nrow=1, ncol=2, byrow=TRUE))
acf(ddiff7, lag=40)
pacf(ddiff7, lag=40)
```
Conclusion, la série différenciée avec le filtre $(I-B^7)$ remplie le critère de stationarité. Il est donc possible d'appliquer un modèle de type $SARIMA(p,d,q)(P,D,Q)_7$ avec $(d,D)=(0,1)$.

# Modélisation

Nous avons vu dans la partie précédante qu'une différanciation d'ordre 1 avec le filtre $(I-B^7)$ donne une série stationnaire. Cela motive l'idée d'appliquer un modèle $SARIMA(p,0,q)\times(Q,1,P). Nous déterminons les coefficients p, q, Q et P avec la méthode `auto.arima`. Il est important d'effectuer la recherche du meilleur modèle avec les paramètres `allowdrift=TRUE` et `include.mean=TRUE` car la différanciation effectuée élimine totalement les tendance d'ordre 1. Le résultat de la recherche est présenté ci-dessous:

```{r, echo=FALSE}
model1 = auto.arima(trainset, d=0, D=1, allowdrift=TRUE, include.mean=TRUE)
model1
```
Pour estimer la pertinance du modèle et en particulier la significativité d'un coefficient, on divise la valeurs de ce derniers par son écart type. Le quotient doit se trouver hors de l'intervalle $[-1.96, 196]$ afin que le paramètre soit considéré comme significatif. Les coefficients ma4, sar1 et drift non sont pas significatifs. Testons alors le modèle $SARIMA(3,0,3)(0,1,1)_7$ sans tendance linéaire.

```{r}
model1 = Arima(trainset, order=c(3,0,3), seasonal=list(order=c(0,1,1), period=7), include.mean=TRUE, include.drift=FALSE)
model1
```
Les coefficients sont tous significatifs. Le score BIC obtenu pour ce modèle est de 1702,45.

Pour considérer que le modèle est bon, les résidus doivent former si possible un bruit blanc. Autrement dit, les résidus réduits doivent être stationnaires, centrés, indépendants et suivre une loi normale. Pour ce faire nous utilisons différants outils statistiques tels que les tests ADF et KPSS présentés précédamment ainsi que les tests de Ljung-Box d'hypothèse nulle $\mathcal{H_0}$ : "La série temporelle ne possède pas d'autocorrélation." et de Shapiro d'hypothèse nulle $\mathcal{H_0}$ : "L'échantillon suit une loi normale."

| Test      | Statistique | p-value   |
|-----------|-------------|-----------|
| ADF       | -11.875     | <0.01     |
| KPSS      | 0.06673     | >0.1      |
| Ljung-Box | -11.875     | <0.01     |
| Shapiro   | 0.06673     | >0.1      |

```{r, warning=FALSE, echo=FALSE}
check_res(model1$residuals, "Analyse de résidus : SARIMA(3,0,3)(0,1,1)[7]")
```


<!-- ## Erreur de prédiction sur testset -->

<!-- Précédement, la comparaison des modèles était basées sur des critères intégrant la notion de parciemonie avec le score BIC et l'étude des coefficients significatif. Nous portions aussi une grande importance à ce que les résidus du modèle soit un bruit blanc gaussien. -->

<!-- Il est temps de comparer nous modèles sur des critères prédictifs. La procédure d'évaluation consiste en une validations simple. Le score utilisé pour comparer les modèles est la RMSE. -->

<!-- ```{r} -->
<!-- n = length(testset) -->

<!-- pred0 = forecast(model0, h=n)$mean + decomp(trainset, testset)$pred -->

<!-- models = c("decomp + SARIMA (d=0, D=1)", -->
<!--            "SARIMA (d=0, D=1)", -->
<!--            "SARIMA (d=1, D=1)", -->
<!--            "log + SARIMA (d=0, D=1)", -->
<!--            "log + SARIMA (d=1, D=1)", -->
<!--            "BoxCox + SARIMA (d=0, D=1)", -->
<!--            "Box-Cox + SARIMA (d=1, D=1)", -->
<!--            "Holt-Winters", -->
<!--            "testset") -->

<!-- pred1 = forecast(model1, h=n) -->
<!-- pred2 = forecast(model2, h=n) -->
<!-- pred3 = forecast(model3, h=n, biasadj=TRUE, lambda = 0) -->
<!-- pred4 = forecast(model4, h=n, biasadj=TRUE, lambda = 0) -->
<!-- pred5 = forecast(model5, h=n, biasadj=TRUE, lambda = -0.7) -->
<!-- pred6 = forecast(model6, h=n, biasadj=TRUE, lambda = -0.7) -->
<!-- pred_hw = forecast(holt_winters, h=n) -->

<!-- # plot(trainset, xlim=c(0,45)) -->
<!-- plot(ts(testset, frequency=7, start=c(41,1)), ylim=c(18, 70)) -->
<!-- lines(ts(pred0, frequency=7, start=c(41,1)), col="gray") -->
<!-- lines(ts(pred1$mean, frequency=7, start=c(41,1)), col="blue") -->
<!-- lines(ts(pred2$mean, frequency=7, start=c(41,1)), col="red") -->
<!-- lines(ts(pred3$mean, frequency=7, start=c(41,1)), col="purple") -->
<!-- lines(ts(pred4$mean, frequency=7, start=c(41,1)), col="orange") -->
<!-- lines(ts(pred5$mean, frequency=7, start=c(41,1)), col="brown") -->
<!-- lines(ts(pred6$mean, frequency=7, start=c(41,1)), col="green") -->
<!-- legend("topleft", legend=models, col=c("gray", "blue", "red", "purple", "orange", "brown", "green", "black"), lw=1, box.lty=0, cex = 0.60) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- perf0 = cbind(type = models[1], getPerformance(as.vector(pred0), as.vector(testset))) -->
<!-- perf1 = cbind(type = models[2], getPerformance(as.vector(pred1$mean), as.vector(testset))) -->
<!-- perf2 = cbind(type = models[3], getPerformance(as.vector(pred2$mean), as.vector(testset))) -->
<!-- perf3 = cbind(type = models[4], getPerformance(as.vector(pred3$mean), as.vector(testset))) -->
<!-- perf4 = cbind(type = models[5], getPerformance(as.vector(pred4$mean), as.vector(testset))) -->
<!-- perf5 = cbind(type = models[6], getPerformance(as.vector(pred5$mean), as.vector(testset))) -->
<!-- perf6 = cbind(type = models[7], getPerformance(as.vector(pred6$mean), as.vector(testset))) -->
<!-- perf_hw = cbind(type = models[8], getPerformance(as.vector(pred_hw$mean), as.vector(testset))) -->

<!-- perf = rbind(perf0, perf1, perf2, perf3, perf4, perf5, perf6, perf_hw) -->

<!-- list(perf[order(perf$RMSE), ], models[order(perf$RMSE)]) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- n = length(testset) -->

<!-- pred0 = forecast(model0, h=n)$mean + pred_decomp(trainset, testset)$pred -->

<!-- models = c("SARIMA (d=0, D=1)", -->
<!--            "log + SARIMA (d=0, D=1)", -->
<!--            "BoxCox + SARIMA (d=0, D=1)", -->
<!--            "testset") -->

<!-- top1 = forecast(model1, h=n) -->
<!-- top2 = forecast(model3, h=n, biasadj=TRUE, lambda = 0) -->
<!-- top3 = forecast(model5, h=n, biasadj=TRUE, lambda = -0.7) -->

<!-- # plot(trainset, xlim=c(0,45)) -->
<!-- plot(ts(testset, frequency=7, start=c(41,1)), ylim=c(18, 60)) -->
<!-- lines(ts(top1$mean, frequency=7, start=c(41,1)), col="blue") -->
<!-- lines(ts(top2$mean, frequency=7, start=c(41,1)), col="red") -->
<!-- lines(ts(top3$mean, frequency=7, start=c(41,1)), col="gray") -->
<!-- legend("topleft", legend=models, col=c("blue", "red", "gray", "black"), lw=1, box.lty=0, cex = .8) -->
<!-- ``` -->

<!-- ## Validation croisée -->

<!-- Précédement, la comparaison des modèles était basées sur des critères intégrant la notion de parciemonie avec le score BIC et l'étude des coefficients significatif. Nous portions aussi une grande importance à ce que les résidus du modèle soit un bruit blanc gaussien. -->

<!-- Il est temps de comparer nous modèles sur des critères prédictifs. -->

<!-- La procédure d'évaluation consiste en une validations croisées adaptées au série temporelle. L'image ci-dessus résume parfaitement l'idée. -->

<!-- ![](https://miro.medium.com/max/1204/1*qvdnPF8ETV9mFdMT0Y_BBA.png) -->

<!-- Le score utilisé pour comparer les modèles est la RMSE. -->


<!-- ## Erreur de prédiction en fonction de l'horizon -->

<!-- lien : https://otexts.com/fpp3/tscv.html -->

<!-- ## Conclusion -->


# Annexe 1

<!-- ```{r} -->
<!-- model0 = auto.arima(decomp_residuals, allowmean=TRUE, allowdrift = TRUE) -->
<!-- model0 -->
<!-- ``` -->
<!-- Tous les coefficient sont significatifs. -->

<!-- ```{r} -->
<!-- plot(decomp_residuals) -->
<!-- lines(model0$fitted, col="red") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- check_stat(model0$residuals) -->
<!-- ``` -->

# Annexe 2

<!-- ```{r} -->
<!-- model1 = auto.arima(trainset, allowmean=TRUE, D=0, d = 1, allowdrift = TRUE) -->
<!-- model1 -->
<!-- ``` -->
<!-- Tous les termes sont significatifs. On obtient un score BIC de 1796.31. -->

<!-- ```{r} -->
<!-- plot(trainset) -->
<!-- lines(model1$fitted, col='red') -->
<!-- ``` -->
<!-- Visuellement, le modèle s'adapte relativement bien aux données. Analysons les résidus de ce premier modèle. -->

<!-- ```{r} -->
<!-- check_stat(model1$residuals) -->
<!-- ``` -->
<!-- #### Conclusion des tests -->
<!-- Stat -->
<!-- Box-Ljung : bb au risque de 5% -->
<!-- KPSS : pas non-stationnaire -->
<!-- ADF : stationnaire -->
<!-- Plot : ras -->
<!-- ACF : pas de pics significatifs -->
<!-- PACF : pas de pics significatifs -->
<!-- Lag 1 : indpt -->

<!-- Normalité -->
<!-- Shapiro : pas gaussien -->
<!-- Histogramme : pas gaussien -->
<!-- Q-Q plot : pas gaussien -->

<!-- Passons en analyse du deuxième modèle. -->

<!-- ```{r} -->
<!-- model2 = auto.arima(trainset, D=1, d=1, allowmean=TRUE, allowdrift=TRUE) -->
<!-- model2 -->
<!-- ``` -->
<!-- Tous les termes sauf sar1 sont significatifs, il serait donc judicieux de tester le modèle $SARIMA(2,1,3)\times(0,1,1)_7$. -->

<!-- ```{r} -->
<!-- model2 = Arima(trainset, order=c(2,1,3), seasonal=list(order=c(0,1,1), period=7), include.mean = TRUE, include.drift = TRUE) -->
<!-- model2 -->
<!-- ``` -->
<!-- Encore une fois, on a un terme non significatif (ar2), testons le modèle $SARIMA(1,1,3)\times(0,1,1)_7$. -->

<!-- ```{r} -->
<!-- model2 = Arima(trainset,order = c(1,1,3), seasonal = list(order=c(0,1,1), period=7), include.mean = TRUE, include.drift = TRUE) -->
<!-- model2 -->
<!-- ``` -->
<!-- Cette troisième tentative semble meilleure du point de vue du BIC avec un score de 1726.91. -->

<!-- ```{r} -->
<!-- plot(trainset) -->
<!-- lines(model2$fitted,col='red') -->
<!-- ``` -->
<!-- Le modèle s'applique relativement bien aux donnée. -->

<!-- Analyse des résidus du modèle 2: -->

<!-- ```{r} -->
<!-- check_stat(model2$residuals) -->
<!-- ``` -->
<!-- #### Conclusion des tests -->
<!-- Stationarité -->
<!-- Box-Ljung : bb avec p-val=0.89 -->
<!-- KPSS : stationnaire -->
<!-- ADF : stationnaire -->
<!-- Plot : pas de saisonalité, semble centrée -->
<!-- ACF : 1 pic légèrement significatif en 10 (posssiblement un artefact) -->
<!-- PACF : 1 pic légèrement significatif en 10 (posssiblement un artefact) -->
<!-- Lag 1 : indpt -->

<!-- Normalité -->
<!-- Shapiro : pas gaussien -->
<!-- Histogramme : pas gaussien -->
<!-- Q-Q plot : pas gaussien -->

<!-- On constate globalement que qu'il est possible d'obtenir des résidus bruit blanc. Cependant, ils ne semblent pas suivre une loi gaussienne. De plus, il possible que notre série soit légèrement hétéroscédastique. Des transformations telles que Box-Cox ou le passage au logarithme peuvent potentiellment atténuer voir corriger ce cela. -->

<!-- ### Transformation de Box-Cox et logarithmique -->

<!-- ```{r} -->
<!-- log_trainset = ts(log(trainset), start=1, frequency = 7) -->
<!-- autoplot(log_trainset) -->
<!-- ``` -->
<!-- On a réussi à bien réduire la variance. Voyons ce que disent les tests ADF et KPSS. -->

<!-- ```{r} -->
<!-- LogTS = log_trainset -->
<!-- DLogTS1 = diff(LogTS) -->
<!-- autoplot(DLogTS1) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- kpss.test(DLogTS1) -->
<!-- adf.test(DLogTS1) -->
<!-- ``` -->
<!-- Les tests vont dans le sens de la stationnarité -->

<!-- ```{r} -->
<!-- acf(as.numeric(DLogTS1), lag.max=40) -->
<!-- pacf(as.numeric(DLogTS1), lag.max=40) -->
<!-- ``` -->
<!-- Mais pas l'ACF où un motif périodique se dessine clairement. -->
<!-- On voit néanmois qu'on a réussi à éliminer la tendance. -->
<!-- Pour éliminer le motif périodique, essayons une différenciation d'ordre 7 -->


<!-- Differenciation I-B⁷ -->
<!-- ```{r} -->
<!-- DLogTS2 = diff(LogTS,7) -->
<!-- plot(DLogTS2) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- kpss.test(DLogTS2) -->
<!-- adf.test(DLogTS2) -->
<!-- ``` -->
<!-- Les tests vont dans le sens de la stationnarité -->

<!-- ```{r} -->
<!-- acf(as.numeric(DLogTS2), lag.max=40) -->
<!-- pacf(as.numeric(DLogTS2), lag.max=40) -->
<!-- ``` -->
<!-- ACF : décroissance rapide avec un pic à 7 -->
<!-- PACF : -->


<!-- Differenciation (I-B)(I-B⁷) -->
<!-- ```{r} -->
<!-- DLogTS3 = diff(diff(LogTS,7)) -->
<!-- plot(DLogTS3) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- kpss.test(DLogTS3) -->
<!-- adf.test(DLogTS3) -->
<!-- ``` -->
<!-- Les tests vont dans le sens de la stationnarité -->

<!-- ```{r} -->
<!-- acf(as.numeric(DLogTS3), lag.max=40) -->
<!-- pacf(as.numeric(DLogTS3), lag.max=40) -->
<!-- ``` -->


<!-- Conclusion : Il est judicieux de considérer les séries DLogTS2 et DLogTS dont la stationnarité est soutenue par l'analyse visuelle et les tests ADF et KPSS. On notera aussi qu'on a éliminé les tendances et les saisonnalités sur ces séries. -->


<!-- ```{r} -->
<!-- model3 = auto.arima(log_trainset, allowmean=TRUE, D=1, d=0, allowdrift=TRUE) -->
<!-- model3 -->
<!-- ``` -->
<!-- Le terme ar1 n'est pas significatif, mais le terme ar2 l'étant, on ne va pas modifier notre modèle -->

<!-- ```{r} -->
<!-- plot(log_trainset) -->
<!-- lines(model3$fitted,col='red') -->
<!-- ``` -->
<!-- Analyse des résidus du modèle. -->

<!-- ```{r} -->
<!-- check_stat(model3$residuals) -->
<!-- ``` -->

<!-- Deuxième modèle -->
<!-- ```{r} -->
<!-- model4 = auto.arima(log_trainset, D=1, d=1, allowdrift=TRUE, allowmean = TRUE) -->
<!-- model4 -->
<!-- ``` -->
<!-- Tout les termes sont significatifs. -->

<!-- Effectuons la même procédure avec la transformation Box-Cox. -->

<!-- ```{r} -->
<!-- bc_trainset = BoxCox(trainset, lambda=-0.7) -->
<!-- plot(bc_trainset) -->
<!-- ``` -->

<!-- Puis appliquons un auto.arima. -->
<!-- ```{r} -->
<!-- model5 = auto.arima(bc_trainset, d=0, D=1, allowdrift=TRUE, allowmean=TRUE) -->
<!-- model5 -->
<!-- ``` -->
<!-- Tous les coefficients sont significatifs. -->

<!-- ```{r} -->
<!-- check_stat(model5$resid) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- model6 = auto.arima(bc_trainset, d=1, D=1, allowdrift=TRUE, allowmean=TRUE) -->
<!-- model6 -->
<!-- ``` -->
<!-- Tous les coefs sont significatifs. -->

<!-- ```{r} -->
<!-- check_stat(model6$residuals) -->
<!-- ``` -->
<!-- ## Holt-Winters -->

<!-- ```{r} -->
<!-- holt_winters = HoltWinters(trainset, seasonal="additive") -->
<!-- plot(holt_winters) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- check_stat(trainset - holt_winters$fitted[,1]) -->
<!-- ``` -->
