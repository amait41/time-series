---
title: "Série Chronologique - Projet"
output:
  pdf_document: 
    toc: yes
    fig_width: 7
    fig_height: 4
    fig_caption: yes
    number_sections: yes
    keep_tex: yes
---

```{r warning=FALSE, echo=FALSE, results='hide'}
library(tseries)
library(forecast)
source(file = "functions.R")
```

# Introduction

L'objet d'étude de ce projet est une série temporelle décrivant le nombre de commandes par jour d'une entreprise angevine de livraison de repas à domicile. Cette série est découpée en un jeu d'entrainement (train set) de 280 observations et un jeu de test (test set) de 21 observations. Notre objectif sera d'étudier cette série et de construire un modèle de la famille des modèles $ARMA$ afin de prédire les valeurs du jeu de test soit 21 jours.

```{r, echo=FALSE, fig.width=8, fig.height=4}
data = read.csv('orders.csv')
trainset = ts(data$n_orders, start=c(1,1), end=c(41, 7), frequency=7)
testset = ts(data$n_orders, start=c(42,1), end=c(44, 7), frequency=7)
plot(trainset, type='l', xlab="Période", ylab="# commades", main="Nombre de commandes par jour (trainset)")
```

# Analyse visuelle

Cette série présente plusieurs caractéristiques remarquables.

Il y a une forte saisonalité sur les jours de la semaine. En effet, le nombre de commandes croit du dimanche au vendredi puis décroît le samedi en moyenne.

La série semble composée de deux voir trois régimes. Un premier lors des 18 premières semaines avant une tendance haussière, une saisonnalité très marquée et une variance relativement grande. Puis vient une période de 16 semaines avec tendance baissière, une saisonnalité moins marquée et une plus faible variance. Enfin, les 5 dernières semaines semblent reprendre le schéma des premières semaines avec un tendance à la hausse, une saisonnalité marquée et une variance qui augmente.

```{r,  echo=FALSE, fig.width=15, fig.height=4}
layout(matrix(c(1, 2), nrow=1, ncol=2, byrow=TRUE))
# Segmentation
t1 = 1:126; X1 = trainset[t1]
t2 = 127:246; X2 = trainset[t2]
t3 = 247:287; X3 = trainset[t3]
lm1 = lm(X1~t1)
lm2 = lm(X2~t2)
lm3 = lm(X3~t3)

plot(t1, X1, type='l', xlab="Index", ylab="# commandes", main="Segmentation de la série temporelle", xlim=c(0,length(trainset)))
lines(t2, X2, type="l", col="blue")
lines(t3, X3, type="l", col="red")
lines(t1, lm1$coefficients[1]+ t1 * lm1$coefficients[2])
lines(t2, lm2$coefficients[1]+ t2 * lm2$coefficients[2], col="blue")
lines(t3, lm3$coefficients[1]+ t3 * lm3$coefficients[2], col="red")

# Saisonalité
help(seasonplot)
seasonplot(trainset, type="l", main="Graphique saisonnier", xlab="Jour", ylab="# commandes", season.labels=c("Dimanche", "Lundi", "Mardi", "Mercredi", "Jeudi", "Vendredi", "Samedi"))
```

Les modèles $ARMA$ font l'hypothèse que la série temporelle est stationnaire (du second ordre), c'est à dire que sa moyenne et sa covariance sont invariantes par translation dans le temps. Pour rendre cette série stationnaire, nous allons utiliser plusieurs approches telles que la décomposition (partie 2) et la différenciation (partie 3). Puis nous établirons des modèles de la famille $ARIMA$ à l'aide des fonctions `auto.arima` et `checkup_res` (partie 3) avant d'évaluer nos modèles sur le jeu de donnée test avec des critères prédictifs (partie 4).

# Décomposition

Afin de valider cette hypothèse, il est possible d'appliquer la procédure suivante. Dans un premier temps, on applique sur la série un opérateur moyenne mobile $M_m(B) = \frac{1}{2m+1} \sum\limits_{n=1}^{m} B^{-k}$ qui élimine les tendances $\tau$-périodiques avec $B$ l'opérateur de retard tel que $B^kX_t=X_{t-k}$. Puis, on estime par régression la tendance sur la série filtrée. Enfin, on estime la composante saisonnière en effectuant la moyenne des périodes et en dupliquant ce résultat autant de fois que nécéssaire. On obtient la décomposition ci-dessous.

```{r, fig.width=7, fig.height=5, echo=FALSE}
decomposition = decompose(trainset)

decomp.plot2 <- function(x, main)
{
    if(missing(main))
	main <- paste("Decomposition of", x$type, "time series")
    plot(cbind(observed = x$random + if (x$type == "additive")
        x$trend + x$seasonal
    else x$trend * x$seasonal, trend = x$trend, seasonal = x$seasonal,
        random = x$random), main = main, xlab="Période")
}
decomp.plot2(decomposition, main="Décomposition additive de la série")
```
Pour pouvoir construire un modèle de prédiction sur la base de ces résultats, nous allons appliquer une régression linéaire sur la tendance puis y ajouter la composante saisonnière.

```{r, fig.width = 12, fig.height=4, echo=FALSE, warning=FALSE}
layout(matrix(c(1, 2), nrow=1, ncol=2, byrow=TRUE))
t = 1:length(decomposition$trend)
t2 = t^2
t3 = t^3
t4 = t^4
reg_lin1 = lm(decomposition$trend ~ t)
reg_lin2 = lm(decomposition$trend ~ t + t2)
reg_lin3 = lm(decomposition$trend ~ t + t2 + t3)
reg_lin4 = lm(decomposition$trend ~ t + t2 + t3 + t4)

plot(t, as.numeric(decomposition$trend), type="l", lwd=1.5, main="Regression sur la tendance", ylab="decomp$trend", xlab="Temps")
lines(reg_lin1$fitted.values, type="l")
lines(reg_lin2$fitted.values, type="l", col="blue")
lines(reg_lin3$fitted.values, type="l", col="red")
lines(reg_lin4$fitted.values, type="l", col="green")
legend("topright", legend=c("ordre 1", "ordre 2", "ordre 3", "ordre 4"), col=c("black", "blue", "red", "green"), lw=1)

decomp_ = decomp(trainset, testset)
plot(decomp_$train, main="Régression + composante saisonière", xlab="Périodes", ylab="decomp$trend + decomp$seasonal")
```
Visuellement, les régressions d'ordre 3 et 4 donnent de très bons résultats avec un R² ajusté de 0.88. Le principe de parcimonie nous pousse à garder la régression linéaire d'ordre 3. Analysons à présent les résidus de la décomposition.

```{r, fig.width=8, fig.height=4, echo=FALSE, warning=FALSE}
plot(decomposition$random, ylab="", xlab="Période", main="Résidus de la décomposition")
```
Il semble y avoir une légère saisonnalité dans les résidus.

En plus de l'aspect visuel, les tests suivants permettent d'évaluer l'hypothèse de stationarité :<br>
- ADF avec $\mathcal{H}_0$ : "la trajectoire est issue d'un processus non stationnaire" contre $\mathcal{H}_1=\bar{\mathcal{H}_0}$.<br>
- KPSS avec $\mathcal{H}_0$ : "la trajectoire est issue d'un processus stationnaire" contre $\mathcal{H}_1=\bar{\mathcal{H}_0}$.<br>

```{r, results="hide", echo=FALSE, warning=FALSE}
res_decomp = decomposition$random[which(!is.na(decomposition$random))]
adf.test(res_decomp)
kpss.test(res_decomp)
```
| Test | Statistique | p-value |
|------|-------------|---------|
| ADF  | -12.369     | <0.01   |
| KPSS | 0.022468    | >0.1    |

Les tests s'accordent. Le test ADF rejette l'hypothèse de non stationarité (p-value<<0.05) alors que le test KPSS ne rejette pas l'hypothèse de stationarité (p-value>>0.05). Nous avons donc obtenu des résidus stionnaires d'après ces tests.

Présentons maintenant deux outils indispensables à l'étude des processus ARMA: l'ACF et la PACF:<br>
- L'ACF représente les autocorrélations entre deux valeurs distantes de $h$ dans le temps. Une autocorrélation nulle à partir d'un rang $q+1$ est caractéristique d'un processus $MA(q)$.<br>
- L'autocorrélogramme PAFC représente quand à lui les corrélation "pur" entre deux valeurs distantes de $h$ dans le temps, c'est à dire entre lesquelles on a supprimé l'influence linéaire des valeurs intermédiaires. Une PACF avec des pics dans le couloir de non-significativité à partir du rang $p+1$ est caractéristique d'un processus $AR(p)$.

De plus, on justifie une tentative de modélisation par un processus $ARMA$ lorsque la série est considérée comme stationnaire et que ses ACF et PACF empiriques montrent une décroissance rapide.

```{r fig.width=12, fig.height=4, echo=FALSE}
layout(matrix(c(1, 2), nrow=1, ncol=2, byrow=TRUE))
acf(res_decomp, main="")
pacf(res_decomp, main="")
```
L'ACF présente des pics périodiques majoritairement significatifs avec une faible décroissance. On observe une corrélation élevée pour les lag multiple de 7. La PACF possède des corrélations partielles significatives aux lags 1 à 6. Ainsi, il est clair que nous encore avons une saisonalité d'ordre 7.

En conclusion, les résidus du modèle de decomposition ne sont pas stationnaires et présentent une saisonalité d'ordre 7.

# Différenciation

## Différenciation simple

Afin de rendre une série stationnaire, il est possible d'intégrer cette dernière en lui appliquant un filtre de la forme $(I-B)$.  On regarde alors si $\Delta X_t = (I-B)X_t$ est stationnaire.

Cette méthode ne donne pas de résultats satisfaisant sur les résidus de la décomposition (cf. Annexe 1). Nous allons alors appliquer cette méthode sur la série originale.

Les tests de stationnarité nous incitent à penser que la série différenciée $(\Delta X_t)$ n'est pas stationnaire.

```{r, echo=FALSE, results='hide', warning=FALSE}
diff1 = diff(trainset, lag=1)
adf.test(diff1)
kpss.test(diff1)
```
| Test      | Statistique | p-value   |
|-----------|-------------|-----------|
| ADF       | -11.875     | <0.01     |
| KPSS      | 0.06673     | >0.1      |

Néanmoins, l'ACF et la PACF présentent repectivement des autocorrélations très significatives sur les lags 7, 14, 21, 28 et 1 à 7.

```{r, fig.width=12, fig.height=4, echo=FALSE}
layout(matrix(c(1, 2), nrow=1, ncol=2, byrow=TRUE))
acf(diff1, main="")
pacf(diff1, main="")
```
Nous en concluons que les incréments ne sont pas stationnaires car ils contiennent encore une composante saisonnière.

## Différentiation saisonnière

La différenciation saisonnière consiste à appliquer un filtre de la forme $(I-B^s)$ à notre série. On obtient alors les incréments saisonniers $(\Delta_s X_t = (I-B^s)X_t)$. L'application de ce filtre à la série semble tout indiquée car ce dernier élimine les tendances périodiques. Les tests ADF et KPPSS s'accordent sur l'hypothèse de stationnarité. 

| Test      | Statistique | p-value   |
|-----------|-------------|-----------|
| ADF       | -7.3894     | <0.01     |
| KPSS      | 0.32541     | >0.1      |

```{r,  echo=FALSE, warning=FALSE, results='hide'}
diff7 = diff(trainset, lag=7)
adf.test(diff7)
kpss.test(diff7)
```
L'ACF montre une décroissance rapide avec des pics significatifs en 1, 4 et 7. La PACF possède aussi une décroissance rapide et des pics significatifs en 1 et 7. On pourra alors tenter une modélisation $ARMA$ sur la série $Y_t = \Delta_7 X_t = (I-B^7) X_t$.

```{r, fig.width=12, fig.height=4, echo=FALSE}
layout(matrix(c(1, 2), nrow=1, ncol=2, byrow=TRUE))
acf(diff7, lag=40, main="")
pacf(diff7, lag=40, main="")
```
Essayons de différencier à nouveau la série, c'est à dire d'appliquer le filtre $(I-B)(I-B^7)$ sur la série de départ.

Les tests vont encore dans le sens de la stationnarité.

| Test      | Statistique | p-value   |
|-----------|-------------|-----------|
| ADF       | -9.6985     | <0.01     |
| KPSS      | 0.011688    | >0.1      |

```{r,  echo=FALSE, warning=FALSE, results='hide'}
ddiff7 = diff(diff(trainset, lag=7))
adf.test(ddiff7)
kpss.test(ddiff7)
```
L'ACF et la PACF donnent de moins bons résultats dans le sens où l'ACF donne des résultats similaires mais la PACF décroit beaucoup moins vite et présente d'avantage de pics significatifs.

```{r, fig.width=12, fig.height=4, echo=FALSE}
layout(matrix(c(1, 2), nrow=1, ncol=2, byrow=TRUE))
acf(ddiff7, lag=40, main="")
pacf(ddiff7, lag=40, main="")
```
Conclusion, la série différenciée avec le filtre $(I-B^7)$ remplit le critère de stationnarité. Il est donc possible d'appliquer un modèle de type $SARIMA(p,d,q)(P,D,Q)_7$ avec $(d,D)=(0,1)$.

# Modélisation

Nous avons vu dans la partie précédente qu'une différenciation d'ordre 1 avec le filtre $(I-B^7)$ donne une série stationnaire. Cela motive l'idée d'appliquer un modèle $SARIMA(p,0,q)(Q,1,P)_7$. Nous déterminons les coefficients p, q, Q et P avec la méthode `auto.arima`. Il est important d'éffectuer la recherche du meilleur modèle avec les paramètres `allowdrift=TRUE` et `include.mean=TRUE` car la différenciation effectuée élimine totalement les tendances d'ordre 1. Le résultat de la recherche est présenté ci-dessous:

```{r, echo=FALSE}
model1 = auto.arima(trainset, d=0, D=1, allowdrift=TRUE, include.mean=TRUE)
model1
```
Pour estimer la pertinence du modèle et en particulier la significativité d'un coefficient, on divise la valeur de ce derniers par son écart type. Le quotient doit se trouver hors de l'intervalle $[-1.96, 1,96]$ pour que le paramètre soit considéré comme significatif. Les coefficients ma4, sar1 et drift ne sont pas significatifs. Testons donc le modèle $SARIMA(3,0,3)(0,1,1)_7$ sans tendance linéaire.

```{r, echo=FALSE}
model1 = auto.arima(trainset, d=0, D=1, max.P=0, include.mean=TRUE)
model1
```
Les coefficients sont tous significatifs sauf le drift. Le score BIC obtenu pour ce modèle est meilleur que pour le modèle précédent avec 1708 contre 1717,88.

Pour considérer le modèle comme satifaisant, les résidus doivent former si possible un bruit blanc. Autrement dit, les résidus réduits doivent être stationnaires, centrés, indépendants et suivre une loi normale. Pour ce faire nous utilisons différents outils statistiques tels que les tests ADF et KPSS présentés précédemment ainsi que les tests de Ljung-Box d'hypothèse nulle $\mathcal{H_0}$ : "La série temporelle ne possède pas d'autocorrélation." ou de Shapiro d'hypothèse nulle $\mathcal{H_0}$ : "L'échantillon suit une loi normale."

| Test          | Statistique | p-value   |
|---------------|-------------|-----------|
| ADF           | -5.7081     | <0.01     |
| KPSS          | 0.18528     | >0.1      |
| Ljung-Box     | 1.9551      | 0.9623    |
| Shapiro       | 0.9132      | 1.671e-11 |

```{r, warning=FALSE, echo=FALSE, results='hide'}
res_model1 = model1$residuals / sd(model1$residuals)
check_res(res_model1, "Analyse de résidus : SARIMA(3,0,3)(0,1,1)[7]")
```
Les résidus de ce modèle 1 semblent tout à fait correspondre à un bruit blanc. En effet, l'ACF et la PACF n'ont aucun pic hors du couloir de non-significativité, les tests ADF et KPSS estiment les résidus stationnaires et le test de Ljung-Box avec une p-value relativement élevée de 0.9628 ne rejette pas l'hypothèse d'indépendance. Néanmoins, leur distribution est légèrement trop concentrée pour coller parfaitement à une distribution normale, comme peut en témoigner la p-value du test de Shapiro.

Ce modèle est assez satisfaisant. D'autres modèles ont été évalués en Annexe 2 et 3, notamment en appliquant des transformations de Box-Cox et logarithmique. L'annexe 4 contient les résultats (non probants) des modélisations exploitant le paramètre `xreg` de la fonction `auto.arima`. Les modèles retenus sont les suivants :

| Modèle                  | Transformation | BIC       |
|-------------------------|----------------|-----------|
| SARIMA(5,0,0)(0,1,1)[7] | Décompositon   |           |
| SARIMA(3,0,3)(0,1,1)[7] | Aucune         | 1702.45   |
| SARIMA(2,0,3)(0,1,1)[7] | Log            | -320.69   |
| SARIMA(2,0,3)(0,1,1)[7] | Box-Cox        | -1185.07  |
| Holt-Winters            | Aucune         | -1185.07  |
| ETS                     | STL            |           |

# Prédiction

Précédemment, la comparaison des modèles était basée sur des critères intégrant la notion de parcimonie avec le score BIC et l'étude des coefficients significatif. Nous portions aussi une grande importance à ce que les résidus du modèle forment un bruit blanc gaussien.

Il est temps de comparer nos modèles sur des critères prédictifs. La procédure d'évaluation consiste en une validation simple. Les scores utilisés pour comparer les modèles sont les métriques RMSE et MAPE (Mean Absolute Percentage Error).

```{r, echo=FALSE, results='hide'}
sarima = Arima(trainset, order=c(3,0,3), seasonal=list(order=c(0,1,1), period=7), include.mean=TRUE, include.drift=FALSE)
log_sarima = Arima(trainset, order=c(2,0,3), seasonal=list(order=c(0,1,1), period=7), include.mean=TRUE, include.drift=FALSE, lambda=0, biasadj=TRUE)
bc_sarima = Arima(trainset, order=c(2,0,3), seasonal=list(order=c(0,1,1), period=7), include.mean=TRUE, include.drift=FALSE, lambda=-0.45, biasadj=TRUE)
arma = auto.arima(decomposition$random)
holt_winters = HoltWinters(trainset, seasonal="additive")
decomp_stl = stl(trainset, s.window=7)

models = c(sarima, log_sarima, bc_sarima, arma, holt_winters, decomp_stl)
models_name = c("SARIMA(3,0,3)(0,1,1)[7]",
                "log + SARIMA(2,0,3)(0,1,1)[7]",
                "BoxCox + SARIMA(2,0,3)(0,1,1)[7]",
                "Décommp. simple + SARIMA(5,0,0)(0,1,1)[7]",
                "Holt-Winters",
                "Décomposition STL",
                "testset")

n2 = length(testset)
pred_sarima = forecast(sarima, h=n2)
pred_log_sarima = forecast(log_sarima, h=n2, biasadj=TRUE, lambda = 0)
pred_bc_sarima = forecast(bc_sarima, h=n2, biasadj=TRUE, lambda = -0.45)
pred_decomp_arma = forecast(arma, h=n2)$mean + decomp(trainset, testset)$pred
pred_hw = forecast(holt_winters, h=n2)
pred_decomp_stl = forecast(stl(trainset, s.window=7), h=n2)

perf_sarima = cbind(type = models_name[1], getPerformance(as.vector(pred_sarima$mean), as.vector(testset)))
perf_log_sarima = cbind(type = models_name[2], getPerformance(as.vector(pred_log_sarima$mean), as.vector(testset)))
perf_bc_sarima = cbind(type = models_name[3], getPerformance(as.vector(pred_bc_sarima$mean), as.vector(testset)))
perf_decomp_arma = cbind(type = models_name[4], getPerformance(as.vector(pred_decomp_arma), as.vector(testset)))
perf_hw = cbind(type = models_name[5], getPerformance(as.vector(pred_hw$mean), as.vector(testset)))
perf_decomp_stl =  cbind(type = models_name[6], getPerformance(as.vector(pred_decomp_stl$mean), as.vector(testset)))

perf = rbind(perf_sarima, perf_log_sarima, perf_bc_sarima, perf_decomp_arma, perf_hw, perf_decomp_stl)

list(perf[order(perf$RMSE), ], models[order(perf$RMSE)])
```

```{r, fig.width=13, fig.height=4, echo=FALSE}
layout(matrix(c(1, 2), nrow=1, ncol=2, byrow=TRUE))
plot(pred_bc_sarima, main="BoxCox + SARIMA(2,0,3)(0,1,1)[7]")
lines(testset)

plot(pred_bc_sarima, xlim=c(38,45), main="BoxCox + SARIMA(2,0,3)(0,1,1)[7], zoom", ylim=c(15,80))
lines(testset)
legend("topleft", legend=c("RMSE = 7.923", "MAPE = 0,179%"))
```

# Conclusion

Grâce aux conclusions des études de la série des parties 1 et 2 nous avons pu construire un modèle $ SARIMA(2,0,3)(0,1,1)_7$ ayant obtenu un score RMSE de 7,923 sur le jeu de données test. Soit une série temporelle $(X_t)$, l'écriture explicite de ce modèle est $(I-\phi_1B - \phi_2B^2)Y_t =(I+\theta_1B+\theta_2B^2+\theta_3B^3)(I+\beta_1B^7)\epsilon_t$ avec $Y_t = (I-B^7)X_t$. On réécrit cela sous la forme $Y_t = \phi_1Y_{t-1} + \phi_2Y_{t-2} + \epsilon_t + \theta_1\epsilon_{t-1} + \theta_2\epsilon_{t-2} + \theta_3\epsilon_{t-3} + \beta_1(\epsilon_{t-7} + \theta_1\epsilon_{t-8} + \theta_2\epsilon_{t-9} + \theta_3\epsilon_{t-10})$ avec 
$$
\begin{equation}
    \begin{cases}
      \phi_1 = \\
        \theta_1 = \\
        \theta_2 = \\
        \theta_3 = \\
        \beta_1 = \\
    \end{cases}       
\end{equation}
$$

Par aileurs, appliquer une transformation de type Box-Cox $B(x, \lambda) = \frac{x^\lambda -1}{\lambda}$ avec un $\lambda=-1$ a amélioré nos résultats. Ce modèle pourrait être challenger par une approche basé sur des techinique d'apprentissage automatique (par exemple en entrainant une forêt aléatoire sur des lags de la série) ou d'apprentissage profond (cf. réseaux de neuronnes LSTM). 

# Annexe 1

L'objectif de cette section est de rendre les résidus $(X_t)$ de la décomposition simple stationnaires.

```{r, warning=FALSE, echo=FALSE}
decomp_res = decompose(trainset)$random
diff1_decomp_res = diff(decomp_res)
check_res(diff1_decomp_res, "Différenciation avec (d,D)=(1,0)")
```
Analyse :

La série $\Delta X_t = (I-B)X_t$ est stationnaire pour les tests ADF et KPSS mais on observe des pics significatifs aux lags 7, 14 et 21 sur l'ACF ainsi que des pics significatifs jusqu'aux lags 9 de la PACF. La distribustion semble suivre une loi normale.

```{r, warning=FALSE, echo=FALSE}
diff7_decomp_res = diff(decomp_res, lag=7)
check_res(diff7_decomp_res, "Différenciation avec (d,D)=(0,1)")
```
Analyse :

L'ACF contient des pics significatifs aux lags 2, 3 et 7 et la PACF aux lags 2, 3, 5, 7, 9 et 10. La distribution ne semble pas normale car trop centré.

```{r, warning=FALSE, echo=FALSE}
ddiff7_decomp_res = diff(diff(decomp_res, lag=7))
check_res(ddiff7_decomp_res, "ddiff7_decomp_res")
```
L'ACF contient des pics légèrement hors du couloir de significativité aux lags 1, 3, 4, 6 et 8 notamment. La PACF obtient des pics significatifs aux lags 1 à 5 et aux lags 9 et 9. La distribution ne semble pas normale car trop centré.

Malgrès ces résultats peu concluant, essayons d'appliquer un modèle sur ces résidus.

```{r}
auto.arima(decomp_res)
```

Tous les coefficients sont significatifs. Le fait d'obtenir $(d,D)=(0,1)$ est cohérant avec l'analyse car la différanciation saisonnière est la tranformation donnant les ACF et PACF les plus convaincantes.


# Annexe 2 : décomposition STL

Une présentation de cette méthode est disponible au lien suivant : https://otexts.com/fpp2/stl.html.

```{r}
decomposition_stl = stl(trainset, s.window=7)
plot(decomposition_stl)
```
# Annexe 3

Dans cette annexe, nous détaillons de façon succincte notre procédure de choix de modèles pour une transformation BoxCox des données. Nous utilisons la fonction `BoxCox.lambda` pour déterminer le meilleur paramètre de transformation BoxCox. Nous utilisons ici cette fonction avec la méthode de Guerrero (1993), où le paramètre déterminé minimise les coefficients de variation $\frac{\sigma}{\mu}$ pour des séries extraites de la série de départ.

```{r, echo=FALSE}
lamb = BoxCox.lambda(trainset, method = 'guerrero')
bc_trainset = BoxCox(trainset, lambda=lamb)
plot(bc_trainset)
```

Une telle transformation n'est pas satisfaisante, on va tenter plusieurs différenciations jusqu'à ce que la série soit stationnaire.

Ainsi, sur les différents incréments de cette série, les tests KPSS et ADF donnent :

|Filtre         | Test      | Statistique | p-value   |
|---------------|-----------|-------------|-----------|
|$(I-B)$        | ADF       | -11.621     | <0.01     |
|$(I-B)$        | KPSS      | 0.10789     | >0.1      |
|$(I-B^7)$      | ADF       | -8.6009     | <0.01     |
|$(I-B^7)$      | KPSS      | 0.27522     | >0.1      |
|$(I-B^7)(I-B)$ | ADF       | -10.188     | <0.01     |
|$(I-B^7)(I-B)$ | KPSS      | 0.011297    | >0.1      |

```{r, echo=FALSE, warning=FALSE, results='hide'}
diff_bc = diff(bc_trainset)
kpss.test(diff_bc)
adf.test(diff_bc)

diff7_bc = diff(bc_trainset,7)
kpss.test(diff7_bc)
adf.test(diff7_bc)

ddiff7_bc = diff(diff(bc_trainset, 7))
kpss.test(ddiff7_bc)
adf.test(ddiff7_bc)
```
Ces tests vont tous dans le sens de la stationnarité. Regardons à présents les ACF et les PACF.

Filtre $(I-B)$:

```{r, fig.width=12, fig.height=4, echo=FALSE}
layout(matrix(c(1, 2), nrow=1, ncol=2, byrow=TRUE))
acf(diff_bc, lag=40, main="")
pacf(diff_bc, lag=40, main="")
```
Un motif périodique se dessine clairement sur l'ACF, cette différenciation n'est satisfaisante.

Filtre $(I-B^7)$:

```{r, fig.width=12, fig.height=4, echo=FALSE}
layout(matrix(c(1, 2), nrow=1, ncol=2, byrow=TRUE))
acf(diff7_bc, lag=40, main="")
pacf(diff7_bc, lag=40, main="")
```
Le résultat est bien meilleur. Il y a un pics significatif au lag 7 sur l'ACF et aux lags 7 et 8 sur la PACF.

Filtre $(I-B)(I-B^7)$:

```{r, fig.width=12, fig.height=4, echo=FALSE}
layout(matrix(c(1, 2), nrow=1, ncol=2, byrow=TRUE))
acf(ddiff7_bc, lag=40, main="")
pacf(ddiff7_bc, lag=40, main="")
```
Des pics significatif sont présent aux lags 1,7 et 8 sur l'ACF et aux lags 1, 2, 3, 6 et 7 sur la PACF.

En conclusion, il est judicieux de considérer les incréments correspondant aux différenciations $I-B^7$ et $(I-B)(I-B^7)$ dont la stationnarité est soutenue par l'analyse visuelle, les tests ADF et KPSS et les graphes ACF et PACF. On notera aussi que la tendance et la saisonnalité est éliminée sur ces séries.

On utilise la fonction `auto.arima` pour déterminer les modèles SARIMA expliquant le mieux nos données.

Un premier modèle avec $d=0$ et $D=1$ ressort :

```{r, echo=FALSE}
auto.arima(trainset, allowmean=TRUE, D=1, d=0, allowdrift=TRUE, lambda=-1, biasadj=TRUE)
```
Les termes ar2, ar3 et ar4 ne sont pas significatifs, on va donc modifier un peu notre modèle.

```{r, echo=FALSE}
auto.arima(bc_trainset, allowmean=TRUE, max.p=1, max.q=1, d=0, D=1, allowdrift=TRUE)
```
Le coefficient sma1 n'est pas significatif, mais sma2 l'est, donc on garde ce modèle qui est meilleur que le précédent au sens du critère BIC. On dispose donc d'un premier modèle SARIMA$(1,0,1)(1,1,2)_7$.


On peut ensuite déterminer un second modèle avec $d=1$ et $D=1$ :
```{r, echo=FALSE}
model4 = auto.arima(bc_trainset, D=1, d=1, allowdrift=TRUE, allowmean = TRUE)
model4
```
Les coefficients ma2 et sma1 ne sont pas significatifs, mais ne nous ammènent pas à rétrograder notre modèle. On dispose donc d'un second modèle SARIMA$(0,1,3)(1,1,2)_7$.


# Annexe 4 : xreg

Cette section a pour but de tester l'ajout d'une régression au sein d'un modèle $SARIMA$.

```{r, echo=FALSE}
t = 1:length(trainset)
t2 = t**2
t3 = t**3
Arima(trainset, order=c(3,0,3), seasonal=list(order=c(0,1,1), period=7), xreg=cbind(t, t2, t3))
Arima(trainset, order=c(3,0,3), seasonal=list(order=c(0,1,1), period=7), xreg=cbind(t, t2))
Arima(trainset, order=c(3,0,3), seasonal=list(order=c(0,1,1), period=7), xreg=cbind(t))
auto.arima(trainset, xreg = cbind(t, t2, t3))
auto.arima(trainset, xreg = cbind(t, t2))
auto.arima(trainset, xreg = cbind(t))
```
On conclue que les coefficients de régression ne sont pas significatifs pour les régressions d'ordre 1, 2 et 3. On ne retient donc pas cette option.